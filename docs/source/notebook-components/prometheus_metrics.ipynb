{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Graphing Prometheus Metrics\n",
    "\n",
    "This notebook demonstrates how to query and graph long-term metrics from a Prometheus server. It assumes that the long-term metrics optional component, as well as the THREDDS log-parser have been enabled.\n",
    "\n",
    "Note: This notebook also requires packages that are not installed by default in the JupyterLab environment. You can install them by running the following command in a terminal:\n",
    "```bash\n",
    "pip install requests_magpie prometheus_pandas prometheus_api_client\n",
    "```\n",
    "\n"
   ],
   "id": "48e00fbf8e27f408"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import datetime as dt\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from requests_magpie import MagpieAuth\n",
    "from prometheus_pandas import query\n",
    "from prometheus_api_client.utils import parse_datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "node = \"https://pavics.ouranos.ca\"\n",
    "# The main production node doesn't yet have the long-term metrics\n",
    "node = \"https://lvupavicsmaster.ouranos.ca\"\n",
    "\n",
    "# PAVICS credentials\n",
    "USERNAME = \"<user>\"\n",
    "PASSWORD = \"<password>\"\n",
    "\n",
    "# Login to Magpie\n",
    "with requests.session() as session:\n",
    "    session.auth = MagpieAuth(f\"{node}/magpie\", USERNAME, PASSWORD)\n",
    "\n",
    "# Connect to Prometheus server\n",
    "# TODO: Change this to the long-term metrics server when available @ /prometheus/federate ?\n",
    "prom = query.Prometheus(api_url=f\"{node}/prometheus\", http=session)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Long-term metrics attributes\n",
    "meta = \\\n",
    "    {'instance:node_cpu_seconds:avg_rate1h_iowait': {\n",
    "        \"name\": \"CPU Load Fraction\",\n",
    "        \"units\": \"\",\n",
    "        \"description\": \"Fraction of the time, over the last hour, that CPUs were working, ie. not idle. 1 means all CPUs were working all the time, 0 means they were all idle all the time\"},\n",
    "        'instance:node_cpu_seconds:avg_rate1h_iowait': {\n",
    "            \"name\": \"CPU IO Wait Fraction\",\n",
    "            \"units\": \"\",\n",
    "            \"description\": \"Fraction of the time, over the last hour, that CPUs were waiting for IO operations to complete.\"},\n",
    "        'instance:go_threads:avg1h': {\n",
    "            \"name\": \"Threads Count\",\n",
    "            \"units\": \"\",\n",
    "            \"description\": \"Number of threads running on the node, averaged over the last hour.\"},\n",
    "        'instance:node_network_transmit_bytes:sum_rate1h': {\n",
    "            \"name\": \"Outgoing Network Transfer\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total number of bytes transmitted over the network by the node, averaged over the last hour.\"},\n",
    "        'instance:node_network_receive_bytes:sum_rate1h': {\n",
    "            \"name\": \"Incoming Network Transfer\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total number of bytes received over the network by the node, averaged over the last hour.\"},\n",
    "        'instance:thredds_transfer_size_bytes:increase1h': {\n",
    "            \"name\": \"THREDDS Download Volume\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total size of data transferred from the THREDDS server, averaged over the last hour.\"},\n",
    "        'instance:node_memory_MemAvailable_bytes:avg1h': {\n",
    "            \"name\": \"Memory Available\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total memory available for use by applications, averaged over the last hour.\"},\n",
    "        'instance:node_memory_SwapFree_bytes:avg1h': {\n",
    "            \"name\": \"Swap Memory Usage\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Swap memory used, averaged over the last hour.\"},\n",
    "        'instance:node_disk_read_bytes:sum_rate1h': {\n",
    "            \"name\": \"Disk Read\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total number of bytes read from disk by the node, averaged over the last hour.\"},\n",
    "        'instance:node_disk_written_bytes:sum_rate1h': {\n",
    "            \"name\": \"Disk Write\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total number of bytes written to disk by the node, averaged over the last hour.\"},\n",
    "        'jupyter:container_fs_writes_bytes:sum_increase1h':{\n",
    "            \"name\": \"Jupyter Container Disk Write\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total number of bytes written to disk by each Jupyter container, averaged over the last hour.\"},\n",
    "        'jupyter:container_cpu_user_seconds:sum_increase1h': {\n",
    "            \"name\": \"Jupyter Container CPU User Time\",\n",
    "            \"units\": \"seconds\",\n",
    "            \"description\": \"Total user CPU time used by each Jupyter container, averaged over the last hour.\"},\n",
    "        \"instance:node_boot_time_seconds:max_over_time1d\": {\n",
    "            \"name\": \"Uptime\",\n",
    "            \"units\": \"seconds\",\n",
    "            \"description\": \"Time since the node was last booted.\"},\n",
    "        'instance:node_filesystem_free_bytes:avg_min_over_time1d': {\n",
    "            \"name\": \"Free Filesystem Space\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total amount of free space on the filesystem, averaged over the last day.\"},\n",
    "        'instance:node_filesystem_size_bytes:avg_max_over_time1d': {\n",
    "            \"name\": \"Total Filesystem Space\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total amount of space on the filesystem, averaged over the last day.\"},\n",
    "        'instance:node_memory_MemTotal_bytes:avg_max_over_time1d': {\n",
    "            \"name\": \"Total Memory\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total amount of memory available on the node, averaged over the last day.\"},\n",
    "        'instance:node_memory_SwapTotal_bytes:avg_min_over_time1d': {\n",
    "            \"name\": \"Total Swap Memory\",\n",
    "            \"units\": \"bytes\",\n",
    "            \"description\": \"Total amount of swap memory available on the node, averaged over the last day.\"},\n",
    "        'jupyter:container_last_seen:sum_rate1d': {\n",
    "            \"name\": \"Jupyter Container Open\",\n",
    "            \"units\": \"\",\n",
    "            \"description\": \"Fraction of the time Jupyter containers were open during the last day.\"}\n",
    "    }"
   ],
   "id": "28311d540223a0f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def get(metric, start=\"1y\", end=\"now\") -> xr.DataArray:\n",
    "    \"\"\"Query a metric over a specified time range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric: str\n",
    "      The Prometheus metric to graph. See `meta` for available long-term metrics.\n",
    "    start: str, datetime\n",
    "        The start time of the time range, can be a relative time from now (e.g. `5d`) or a specific date (YYYY-MM-DD).\n",
    "    end: str, datetime\n",
    "        The end time of the time range, can be `now` or a specific date (YYYY-MM-DD).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Time series of the metric over the specified time range.\n",
    "    \"\"\"\n",
    "    # We need to use `last_over_time` to get the last value of the metric over the specified time range,\n",
    "    # otherwise prometheus won't return all the existing records. By default, a Prometheus time stamp is\n",
    "    # valid during 5 minutes.\n",
    "    if \"1h\" in metric:\n",
    "        step = dt.timedelta(hours=1)\n",
    "        metric_ = f\"last_over_time({metric}[1h])\"\n",
    "    elif \"1d\" in metric:\n",
    "        step = dt.timedelta(days=1)\n",
    "        metric_ = f\"last_over_time({metric}[1d])\"\n",
    "    else:\n",
    "        step = \"5m\"\n",
    "        metric_ = metric\n",
    "\n",
    "    if type(start) is str:\n",
    "        start = parse_datetime(start)\n",
    "\n",
    "    if type(end) is str:\n",
    "        end = parse_datetime(end)\n",
    "\n",
    "    df = prom.query_range(metric_, start=start, end=end, step=step)\n",
    "    return to_dataarray(df, metric)\n",
    "\n",
    "\n",
    "def to_dataarray(df: pd.DataFrame, metric) -> xr.DataArray:\n",
    "    \"\"\"Convert a DataFrame from a Prometheus query to a DataArray.\n",
    "\n",
    "    The function includes metadata attributes defined above, and creates coordinates from the records labels.\n",
    "    \"\"\"\n",
    "    ds = df.to_xarray().rename(index=\"time\")\n",
    "\n",
    "    # Match record labels\n",
    "    pat = re.compile(r'(\\w+)=\"([^\"]+)\"')\n",
    "\n",
    "    das = []\n",
    "    for name, da in ds.data_vars.items():\n",
    "        # Set the name and description of the data array\n",
    "        da.name = metric\n",
    "        da.attrs[\"long_name\"] = meta[metric][\"name\"]\n",
    "        da.attrs[\"units\"] = meta[metric][\"units\"]\n",
    "        da.attrs[\"description\"] = meta[metric][\"description\"]\n",
    "\n",
    "        # Convert labels into coordinates\n",
    "        labels = re.findall(pat, name)\n",
    "        coords = {key: [value] for key, value in labels}\n",
    "\n",
    "        das.append(da.expand_dims(coords))\n",
    "\n",
    "    return xr.combine_by_coords(das).squeeze()[metric]\n",
    "\n",
    "\n",
    "def graph_metric_ts(da: xr.DataArray) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Graph a simple one-dimensional metric over a specified time range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    da: xr.DataArray\n",
    "        A time series of the metric to graph.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    da.plot(add_legend=False, ax=ax)\n",
    "\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    label = da.attrs[\"long_name\"]\n",
    "    if u:= da.attrs.get(\"units\"):\n",
    "        label += f\" ({u})\"\n",
    "\n",
    "    ax.set_ylabel(label)\n",
    "\n",
    "    return fig\n"
   ],
   "id": "66e1d7fe3fa1a42e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = graph_metric_ts(get(\"instance:node_cpu_seconds:avg_rate1h_iowait\"))",
   "id": "9b3db717d4ca1353",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Bar graph of the most downloaded datasets in the last 120 days\n",
    "def graph_most_downloaded(start=\"1y\", end=\"now\", n=5) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Graph the most downloaded datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start: str, datetime\n",
    "        The start time of the time range, can be a relative time from now (e.g. `5d`) or a specific date (YYYY-MM-DD).\n",
    "    end: str, datetime\n",
    "        The end time of the time range, can be `now` or a specific date (YYYY-MM-DD).\n",
    "    n : int\n",
    "        The number of datasets to display in the graph.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    da = get(\"instance:thredds_transfer_size_bytes:increase1h\", start=start, end=end)\n",
    "    df = da.sum([\"remote_addr\", \"tds_service\", \"variable\", \"time\"]).to_series()\n",
    "\n",
    "    # Then we sum the time series for each dataset to get the total size of data transferred.\n",
    "    tds = df.sort_values(ascending=False)[n:0:-1] / 1024**2\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax = tds.plot.barh(title=f\"Top {n} downloaded datasets\", ax=ax)\n",
    "    ax.set_xlabel(\"Data transferred (MB)\")\n",
    "\n",
    "    return fig"
   ],
   "id": "1c7e02e83b5e98bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = graph_most_downloaded()\n",
   "id": "f98daa304afc2c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fdfd006202f8658c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
